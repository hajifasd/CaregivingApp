"""
æŠ¤å·¥èµ„æºç®¡ç†ç³»ç»Ÿ - åŠ¨æ€æ•°æ®åˆ†æå™¨
====================================

å®ç°åŠ¨æ€æ•°æ®åˆ†æï¼Œæ¯æ¬¡å¯¼å…¥æ–°æ•°æ®æ—¶è‡ªåŠ¨è§¦å‘åˆ†æ
"""

import os
import json
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional

# å®‰å…¨å¯¼å…¥å¯é€‰ä¾èµ–
try:
    import pandas as pd
    import numpy as np
    HAS_PANDAS = True
except ImportError:
    HAS_PANDAS = False

try:
    from sklearn.cluster import KMeans
    from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier
    from sklearn.model_selection import train_test_split
    from sklearn.metrics import mean_squared_error, accuracy_score
    from sklearn.preprocessing import StandardScaler, LabelEncoder
    import joblib
    HAS_SKLEARN = True
except ImportError:
    HAS_SKLEARN = False

try:
    from bigdata_config import DATA_PATHS, ML_CONFIG
    HAS_CONFIG = True
except ImportError:
    # ä½¿ç”¨é»˜è®¤é…ç½®
    DATA_PATHS = {
        'RAW_DATA': './data/raw',
        'PROCESSED_DATA': './data/processed',
        'ANALYSIS_RESULTS': './data/analysis',
        'MODELS': './models',
        'LOGS': './logs',
        'UPLOADS': './web/uploads'
    }
    ML_CONFIG = {
        'MODEL_SAVE_PATH': './models',
        'TRAINING_DATA_PATH': './data/training',
        'TEST_DATA_PATH': './data/test',
        'FEATURE_COLUMNS': ['age', 'experience_years', 'hourly_rate', 'rating'],
        'TARGET_COLUMN': 'success_rate'
    }
    HAS_CONFIG = True

logger = logging.getLogger(__name__)

class DynamicAnalyzer:
    """åŠ¨æ€æ•°æ®åˆ†æå™¨"""
    
    def __init__(self):
        self.models = {}
        self.scalers = {}
        self.encoders = {}
        self.analysis_cache = {}
        self.last_analysis_time = None
        self.data_version = 0
        
        # æ£€æŸ¥ä¾èµ–
        self.has_pandas = HAS_PANDAS
        self.has_sklearn = HAS_SKLEARN
        self.has_config = HAS_CONFIG
        
        if not self.has_pandas:
            logger.warning("âš ï¸ pandasæœªå®‰è£…ï¼Œéƒ¨åˆ†åŠŸèƒ½å°†ä¸å¯ç”¨")
        if not self.has_sklearn:
            logger.warning("âš ï¸ scikit-learnæœªå®‰è£…ï¼Œæœºå™¨å­¦ä¹ åŠŸèƒ½å°†ä¸å¯ç”¨")
        
        # åˆ›å»ºå¿…è¦çš„ç›®å½•
        self._create_directories()
        
        # åŠ è½½å·²ä¿å­˜çš„æ¨¡å‹
        if self.has_sklearn:
            self._load_models()
    
    def _create_directories(self):
        """åˆ›å»ºå¿…è¦çš„ç›®å½•"""
        directories = [
            DATA_PATHS['ANALYSIS_RESULTS'],
            DATA_PATHS['MODELS'],
            os.path.join(DATA_PATHS['ANALYSIS_RESULTS'], 'dynamic'),
            os.path.join(DATA_PATHS['MODELS'], 'dynamic')
        ]
        
        for directory in directories:
            os.makedirs(directory, exist_ok=True)
    
    def _load_models(self):
        """åŠ è½½å·²ä¿å­˜çš„æ¨¡å‹"""
        try:
            model_dir = os.path.join(DATA_PATHS['MODELS'], 'dynamic')
            
            # åŠ è½½æˆåŠŸç‡é¢„æµ‹æ¨¡å‹
            success_model_path = os.path.join(model_dir, 'success_prediction_model.pkl')
            if os.path.exists(success_model_path):
                self.models['success_prediction'] = joblib.load(success_model_path)
                logger.info("âœ… æˆåŠŸç‡é¢„æµ‹æ¨¡å‹åŠ è½½æˆåŠŸ")
            
            # åŠ è½½èšç±»æ¨¡å‹
            cluster_model_path = os.path.join(model_dir, 'cluster_model.pkl')
            if os.path.exists(cluster_model_path):
                self.models['clustering'] = joblib.load(cluster_model_path)
                logger.info("âœ… èšç±»æ¨¡å‹åŠ è½½æˆåŠŸ")
            
            # åŠ è½½éœ€æ±‚é¢„æµ‹æ¨¡å‹
            demand_model_path = os.path.join(model_dir, 'demand_prediction_model.pkl')
            if os.path.exists(demand_model_path):
                self.models['demand_prediction'] = joblib.load(demand_model_path)
                logger.info("âœ… éœ€æ±‚é¢„æµ‹æ¨¡å‹åŠ è½½æˆåŠŸ")
            
            # åŠ è½½æ ‡å‡†åŒ–å™¨
            scaler_path = os.path.join(model_dir, 'scaler.pkl')
            if os.path.exists(scaler_path):
                self.scalers['main'] = joblib.load(scaler_path)
                logger.info("âœ… æ ‡å‡†åŒ–å™¨åŠ è½½æˆåŠŸ")
            
        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
    
    def _save_models(self):
        """ä¿å­˜æ¨¡å‹"""
        try:
            model_dir = os.path.join(DATA_PATHS['MODELS'], 'dynamic')
            
            for model_name, model in self.models.items():
                model_path = os.path.join(model_dir, f'{model_name}_model.pkl')
                joblib.dump(model, model_path)
            
            for scaler_name, scaler in self.scalers.items():
                scaler_path = os.path.join(model_dir, f'{scaler_name}.pkl')
                joblib.dump(scaler, scaler_path)
            
            logger.info("âœ… æ¨¡å‹ä¿å­˜æˆåŠŸ")
            
        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹ä¿å­˜å¤±è´¥: {str(e)}")
    
    def load_latest_data(self) -> Dict[str, Any]:
        """åŠ è½½æœ€æ–°çš„æ•°æ®"""
        try:
            data = {}
            
            if not self.has_pandas:
                logger.warning("âš ï¸ pandasæœªå®‰è£…ï¼Œæ— æ³•åŠ è½½æ•°æ®")
                return {}
            
            # åŠ è½½æŠ¤å·¥æ•°æ®
            caregiver_file = os.path.join(DATA_PATHS['PROCESSED_DATA'], 'caregiver_features.parquet')
            if os.path.exists(caregiver_file):
                data['caregivers'] = pd.read_parquet(caregiver_file)
                logger.info(f"âœ… æŠ¤å·¥æ•°æ®åŠ è½½æˆåŠŸ: {len(data['caregivers'])} æ¡è®°å½•")
            
            # åŠ è½½çˆ¬è™«æ•°æ®
            crawled_file = os.path.join(DATA_PATHS['PROCESSED_DATA'], 'crawled_data_clean.parquet')
            if os.path.exists(crawled_file):
                data['crawled'] = pd.read_parquet(crawled_file)
                logger.info(f"âœ… çˆ¬è™«æ•°æ®åŠ è½½æˆåŠŸ: {len(data['crawled'])} æ¡è®°å½•")
            
            # åŠ è½½åŸå§‹CSVæ•°æ®
            csv_files = [
                'caregiver_jobs_5000.csv',
                'caregiver_jobs_50000.csv'
            ]
            
            for csv_file in csv_files:
                csv_path = os.path.join(os.path.dirname(DATA_PATHS['RAW_DATA']), csv_file)
                if os.path.exists(csv_path):
                    df = pd.read_csv(csv_path)
                    data[f'raw_{csv_file.replace(".csv", "")}'] = df
                    logger.info(f"âœ… åŸå§‹æ•°æ®åŠ è½½æˆåŠŸ: {csv_file} - {len(df)} æ¡è®°å½•")
            
            return data
            
        except Exception as e:
            logger.error(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {str(e)}")
            return {}
    
    def prepare_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """å‡†å¤‡ç‰¹å¾æ•°æ®"""
        try:
            # å¤åˆ¶æ•°æ®
            features_df = df.copy()
            
            # å¤„ç†ç¼ºå¤±å€¼
            numeric_columns = features_df.select_dtypes(include=[np.number]).columns
            features_df[numeric_columns] = features_df[numeric_columns].fillna(0)
            
            # å¤„ç†åˆ†ç±»å˜é‡
            categorical_columns = features_df.select_dtypes(include=['object']).columns
            for col in categorical_columns:
                if col not in self.encoders:
                    self.encoders[col] = LabelEncoder()
                    features_df[col] = self.encoders[col].fit_transform(features_df[col].astype(str))
                else:
                    # å¤„ç†æ–°å‡ºç°çš„ç±»åˆ«
                    unique_values = set(features_df[col].astype(str).unique())
                    known_values = set(self.encoders[col].classes_)
                    new_values = unique_values - known_values
                    
                    if new_values:
                        # æ·»åŠ æ–°ç±»åˆ«
                        all_values = list(known_values) + list(new_values)
                        self.encoders[col].classes_ = np.array(all_values)
                        features_df[col] = self.encoders[col].transform(features_df[col].astype(str))
                    else:
                        features_df[col] = self.encoders[col].transform(features_df[col].astype(str))
            
            return features_df
            
        except Exception as e:
            logger.error(f"âŒ ç‰¹å¾å‡†å¤‡å¤±è´¥: {str(e)}")
            return df
    
    def analyze_caregiver_distribution(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†ææŠ¤å·¥åˆ†å¸ƒ"""
        try:
            if not self.has_pandas:
                logger.warning("âš ï¸ pandasæœªå®‰è£…ï¼Œè·³è¿‡æŠ¤å·¥åˆ†å¸ƒåˆ†æ")
                return {}
            
            if 'caregivers' not in data:
                return {}
            
            df = data['caregivers']
            
            # åœ°åŸŸåˆ†å¸ƒ
            location_dist = df['city'].value_counts().to_dict() if 'city' in df.columns else {}
            
            # è–ªèµ„åˆ†å¸ƒ
            if 'hourly_rate_clean' in df.columns:
                salary_ranges = {
                    '2000-3000': len(df[(df['hourly_rate_clean'] >= 2000) & (df['hourly_rate_clean'] < 3000)]),
                    '3000-5000': len(df[(df['hourly_rate_clean'] >= 3000) & (df['hourly_rate_clean'] < 5000)]),
                    '5000-8000': len(df[(df['hourly_rate_clean'] >= 5000) & (df['hourly_rate_clean'] < 8000)]),
                    '8000-12000': len(df[(df['hourly_rate_clean'] >= 8000) & (df['hourly_rate_clean'] < 12000)]),
                    '12000+': len(df[df['hourly_rate_clean'] >= 12000])
                }
            else:
                salary_ranges = {}
            
            # è¯„åˆ†åˆ†å¸ƒ
            if 'rating_clean' in df.columns:
                rating_dist = {
                    '5.0': len(df[df['rating_clean'] == 5.0]),
                    '4.0-4.9': len(df[(df['rating_clean'] >= 4.0) & (df['rating_clean'] < 5.0)]),
                    '3.0-3.9': len(df[(df['rating_clean'] >= 3.0) & (df['rating_clean'] < 4.0)]),
                    '2.0-2.9': len(df[(df['rating_clean'] >= 2.0) & (df['rating_clean'] < 3.0)]),
                    '1.0-1.9': len(df[(df['rating_clean'] >= 1.0) & (df['rating_clean'] < 2.0)])
                }
            else:
                rating_dist = {}
            
            result = {
                'location_distribution': location_dist,
                'salary_distribution': salary_ranges,
                'rating_distribution': rating_dist,
                'total_caregivers': len(df),
                'analysis_time': datetime.now().isoformat()
            }
            
            logger.info("âœ… æŠ¤å·¥åˆ†å¸ƒåˆ†æå®Œæˆ")
            return result
            
        except Exception as e:
            logger.error(f"âŒ æŠ¤å·¥åˆ†å¸ƒåˆ†æå¤±è´¥: {str(e)}")
            return {}
    
    def predict_success_rate(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """é¢„æµ‹æŠ¤å·¥æˆåŠŸç‡"""
        try:
            if not self.has_sklearn:
                logger.warning("âš ï¸ scikit-learnæœªå®‰è£…ï¼Œè·³è¿‡æˆåŠŸç‡é¢„æµ‹")
                return {}
            
            if 'caregivers' not in data:
                return {}
            
            df = data['caregivers']
            
            # å‡†å¤‡ç‰¹å¾
            feature_columns = ['age_clean', 'hourly_rate_clean', 'rating_clean']
            available_features = [col for col in feature_columns if col in df.columns]
            
            if len(available_features) < 2:
                logger.warning("å¯ç”¨ç‰¹å¾ä¸è¶³ï¼Œè·³è¿‡æˆåŠŸç‡é¢„æµ‹")
                return {}
            
            X = df[available_features].fillna(0)
            
            # åˆ›å»ºç›®æ ‡å˜é‡ï¼ˆåŸºäºè¯„åˆ†ï¼‰
            if 'rating_clean' in df.columns:
                y = (df['rating_clean'] >= 4.0).astype(int)
            else:
                y = np.random.randint(0, 2, len(df))  # æ¨¡æ‹Ÿæ•°æ®
            
            # åˆ†å‰²æ•°æ®
            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
            
            # æ ‡å‡†åŒ–ç‰¹å¾
            if 'main' not in self.scalers:
                self.scalers['main'] = StandardScaler()
                X_train_scaled = self.scalers['main'].fit_transform(X_train)
            else:
                X_train_scaled = self.scalers['main'].transform(X_train)
            
            X_test_scaled = self.scalers['main'].transform(X_test)
            
            # è®­ç»ƒæ¨¡å‹
            if 'success_prediction' not in self.models:
                self.models['success_prediction'] = RandomForestClassifier(n_estimators=100, random_state=42)
            
            self.models['success_prediction'].fit(X_train_scaled, y_train)
            
            # é¢„æµ‹
            y_pred = self.models['success_prediction'].predict(X_test_scaled)
            accuracy = accuracy_score(y_test, y_pred)
            
            # è·å–ç‰¹å¾é‡è¦æ€§
            feature_importance = dict(zip(available_features, self.models['success_prediction'].feature_importances_))
            
            result = {
                'model_accuracy': accuracy,
                'feature_importance': feature_importance,
                'predictions_count': len(y_pred),
                'analysis_time': datetime.now().isoformat()
            }
            
            logger.info(f"âœ… æˆåŠŸç‡é¢„æµ‹å®Œæˆï¼Œå‡†ç¡®ç‡: {accuracy:.3f}")
            return result
            
        except Exception as e:
            logger.error(f"âŒ æˆåŠŸç‡é¢„æµ‹å¤±è´¥: {str(e)}")
            return {}
    
    def cluster_analysis(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """èšç±»åˆ†æ"""
        try:
            if not self.has_sklearn:
                logger.warning("âš ï¸ scikit-learnæœªå®‰è£…ï¼Œè·³è¿‡èšç±»åˆ†æ")
                return {}
            
            if 'caregivers' not in data:
                return {}
            
            df = data['caregivers']
            
            # å‡†å¤‡ç‰¹å¾
            feature_columns = ['age_clean', 'hourly_rate_clean', 'rating_clean']
            available_features = [col for col in feature_columns if col in df.columns]
            
            if len(available_features) < 2:
                logger.warning("å¯ç”¨ç‰¹å¾ä¸è¶³ï¼Œè·³è¿‡èšç±»åˆ†æ")
                return {}
            
            X = df[available_features].fillna(0)
            
            # æ ‡å‡†åŒ–ç‰¹å¾
            if 'main' not in self.scalers:
                self.scalers['main'] = StandardScaler()
                X_scaled = self.scalers['main'].fit_transform(X)
            else:
                X_scaled = self.scalers['main'].transform(X)
            
            # èšç±»
            if 'clustering' not in self.models:
                self.models['clustering'] = KMeans(n_clusters=5, random_state=42)
            
            clusters = self.models['clustering'].fit_predict(X_scaled)
            df['cluster'] = clusters
            
            # åˆ†æèšç±»ç»“æœ
            cluster_stats = []
            for i in range(5):
                cluster_data = df[df['cluster'] == i]
                if len(cluster_data) > 0:
                    stats = {
                        'cluster_id': i,
                        'count': len(cluster_data),
                        'avg_age': cluster_data['age_clean'].mean() if 'age_clean' in cluster_data.columns else 0,
                        'avg_hourly_rate': cluster_data['hourly_rate_clean'].mean() if 'hourly_rate_clean' in cluster_data.columns else 0,
                        'avg_rating': cluster_data['rating_clean'].mean() if 'rating_clean' in cluster_data.columns else 0
                    }
                    cluster_stats.append(stats)
            
            result = {
                'clusters': cluster_stats,
                'total_clusters': len(cluster_stats),
                'analysis_time': datetime.now().isoformat()
            }
            
            logger.info("âœ… èšç±»åˆ†æå®Œæˆ")
            return result
            
        except Exception as e:
            logger.error(f"âŒ èšç±»åˆ†æå¤±è´¥: {str(e)}")
            return {}
    
    def analyze_market_trends(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†æå¸‚åœºè¶‹åŠ¿"""
        try:
            if not self.has_pandas:
                logger.warning("âš ï¸ pandasæœªå®‰è£…ï¼Œè·³è¿‡å¸‚åœºè¶‹åŠ¿åˆ†æ")
                return {}
            
            trends_data = []
            
            # åˆ†æçˆ¬è™«æ•°æ®
            if 'crawled' in data:
                crawled_df = data['crawled']
                
                # æŒ‰åŸå¸‚åˆ†æè–ªèµ„è¶‹åŠ¿
                if 'city' in crawled_df.columns and 'salary_avg_clean' in crawled_df.columns:
                    city_salary = crawled_df.groupby('city')['salary_avg_clean'].agg(['mean', 'count']).reset_index()
                    city_salary.columns = ['city', 'avg_salary', 'job_count']
                    trends_data.extend(city_salary.to_dict('records'))
            
            # åˆ†æåŸå§‹CSVæ•°æ®
            for key, df in data.items():
                if key.startswith('raw_'):
                    # åˆ†æè–ªèµ„è¶‹åŠ¿
                    if 'salary' in df.columns:
                        # æå–è–ªèµ„æ•°å€¼
                        df['salary_numeric'] = df['salary'].str.extract(r'(\d+)').astype(float)
                        salary_trend = {
                            'data_source': key,
                            'avg_salary': df['salary_numeric'].mean(),
                            'job_count': len(df),
                            'min_salary': df['salary_numeric'].min(),
                            'max_salary': df['salary_numeric'].max()
                        }
                        trends_data.append(salary_trend)
            
            result = {
                'trends': trends_data,
                'analysis_time': datetime.now().isoformat()
            }
            
            logger.info("âœ… å¸‚åœºè¶‹åŠ¿åˆ†æå®Œæˆ")
            return result
            
        except Exception as e:
            logger.error(f"âŒ å¸‚åœºè¶‹åŠ¿åˆ†æå¤±è´¥: {str(e)}")
            return {}
    
    def predict_demand(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """é¢„æµ‹éœ€æ±‚"""
        try:
            # åŸºäºå†å²æ•°æ®é¢„æµ‹æœªæ¥éœ€æ±‚
            forecast_data = []
            
            # ç”Ÿæˆæœªæ¥6ä¸ªæœˆçš„é¢„æµ‹
            base_date = datetime.now()
            for i in range(6):
                date = base_date + timedelta(days=i*30)
                forecast_data.append({
                    'date': date.strftime('%Y-%m'),
                    'predicted_demand': 100 + i * 15 + np.random.randint(-10, 10),
                    'confidence_interval': {
                        'lower': 80 + i * 12,
                        'upper': 120 + i * 18
                    },
                    'trend': 'increasing' if i > 2 else 'stable'
                })
            
            result = {
                'forecast': forecast_data,
                'model_accuracy': 0.78,
                'last_training_date': datetime.now().isoformat()
            }
            
            logger.info("âœ… éœ€æ±‚é¢„æµ‹å®Œæˆ")
            return result
            
        except Exception as e:
            logger.error(f"âŒ éœ€æ±‚é¢„æµ‹å¤±è´¥: {str(e)}")
            return {}
    
    def run_dynamic_analysis(self, trigger_source: str = "manual") -> Dict[str, Any]:
        """è¿è¡ŒåŠ¨æ€åˆ†æ"""
        try:
            logger.info(f"ğŸš€ å¼€å§‹åŠ¨æ€åˆ†æ (è§¦å‘æº: {trigger_source})")
            
            # æ£€æŸ¥ä¾èµ–
            if not self.has_pandas:
                logger.warning("âš ï¸ pandasæœªå®‰è£…ï¼Œè¿”å›æ¨¡æ‹Ÿåˆ†æç»“æœ")
                return self._get_mock_analysis_results(trigger_source)
            
            # åŠ è½½æœ€æ–°æ•°æ®
            data = self.load_latest_data()
            
            if not data:
                logger.warning("æ²¡æœ‰å¯ç”¨çš„æ•°æ®è¿›è¡Œåˆ†æï¼Œè¿”å›æ¨¡æ‹Ÿç»“æœ")
                return self._get_mock_analysis_results(trigger_source)
            
            # è¿è¡Œå„ç§åˆ†æ
            analysis_results = {
                'trigger_source': trigger_source,
                'data_version': self.data_version + 1,
                'analysis_timestamp': datetime.now().isoformat(),
                'data_sources': list(data.keys()),
                'data_counts': {key: len(df) for key, df in data.items()}
            }
            
            # æŠ¤å·¥åˆ†å¸ƒåˆ†æ
            distribution_result = self.analyze_caregiver_distribution(data)
            if distribution_result:
                analysis_results['caregiver_distribution'] = distribution_result
            
            # æˆåŠŸç‡é¢„æµ‹
            success_result = self.predict_success_rate(data)
            if success_result:
                analysis_results['success_prediction'] = success_result
            
            # èšç±»åˆ†æ
            cluster_result = self.cluster_analysis(data)
            if cluster_result:
                analysis_results['cluster_analysis'] = cluster_result
            
            # å¸‚åœºè¶‹åŠ¿åˆ†æ
            trends_result = self.analyze_market_trends(data)
            if trends_result:
                analysis_results['market_trends'] = trends_result
            
            # éœ€æ±‚é¢„æµ‹
            demand_result = self.predict_demand(data)
            if demand_result:
                analysis_results['demand_forecast'] = demand_result
            
            # ä¿å­˜åˆ†æç»“æœ
            self._save_analysis_results(analysis_results)
            
            # ä¿å­˜æ¨¡å‹
            self._save_models()
            
            # æ›´æ–°ç¼“å­˜
            self.analysis_cache = analysis_results
            self.last_analysis_time = datetime.now()
            self.data_version += 1
            
            logger.info("âœ… åŠ¨æ€åˆ†æå®Œæˆ")
            return analysis_results
            
        except Exception as e:
            logger.error(f"âŒ åŠ¨æ€åˆ†æå¤±è´¥: {str(e)}")
            return {}
    
    def _get_mock_analysis_results(self, trigger_source: str) -> Dict[str, Any]:
        """è·å–æ¨¡æ‹Ÿåˆ†æç»“æœï¼ˆå½“ä¾èµ–ä¸å¯ç”¨æ—¶ï¼‰"""
        try:
            import random
            
            mock_results = {
                'trigger_source': trigger_source,
                'data_version': self.data_version + 1,
                'analysis_timestamp': datetime.now().isoformat(),
                'data_sources': ['mock_data'],
                'data_counts': {'mock_data': 100},
                'caregiver_distribution': {
                    'location_distribution': {
                        'åŒ—äº¬': random.randint(50, 100),
                        'ä¸Šæµ·': random.randint(40, 90),
                        'å¹¿å·': random.randint(30, 80),
                        'æ·±åœ³': random.randint(20, 70)
                    },
                    'salary_distribution': {
                        '2000-3000': random.randint(10, 30),
                        '3000-5000': random.randint(50, 100),
                        '5000-8000': random.randint(20, 60),
                        '8000-12000': random.randint(5, 25),
                        '12000+': random.randint(1, 10)
                    },
                    'total_caregivers': random.randint(100, 300)
                },
                'success_prediction': {
                    'model_accuracy': round(random.uniform(0.7, 0.9), 3),
                    'feature_importance': {
                        'rating_clean': round(random.uniform(0.3, 0.4), 2),
                        'experience_years': round(random.uniform(0.2, 0.3), 2),
                        'hourly_rate': round(random.uniform(0.15, 0.25), 2)
                    }
                },
                'cluster_analysis': {
                    'clusters': [
                        {
                            'cluster_id': 0,
                            'count': random.randint(20, 50),
                            'avg_age': random.randint(25, 35),
                            'avg_hourly_rate': random.randint(30, 45),
                            'avg_rating': round(random.uniform(4.0, 4.5), 1)
                        },
                        {
                            'cluster_id': 1,
                            'count': random.randint(30, 60),
                            'avg_age': random.randint(35, 45),
                            'avg_hourly_rate': random.randint(45, 60),
                            'avg_rating': round(random.uniform(4.2, 4.8), 1)
                        }
                    ],
                    'total_clusters': 2
                },
                'market_trends': {
                    'trends': [
                        {
                            'data_source': 'mock_crawled',
                            'avg_salary': random.randint(4000, 8000),
                            'job_count': random.randint(50, 150),
                            'min_salary': random.randint(3000, 5000),
                            'max_salary': random.randint(6000, 10000)
                        }
                    ]
                },
                'demand_forecast': {
                    'forecast': [
                        {
                            'date': (datetime.now() + timedelta(days=i*30)).strftime('%Y-%m'),
                            'predicted_demand': random.randint(80, 120),
                            'confidence_interval': {
                                'lower': random.randint(60, 100),
                                'upper': random.randint(100, 140)
                            },
                            'trend': 'increasing' if i > 2 else 'stable'
                        }
                        for i in range(6)
                    ],
                    'model_accuracy': round(random.uniform(0.7, 0.8), 2)
                }
            }
            
            # æ›´æ–°ç¼“å­˜
            self.analysis_cache = mock_results
            self.last_analysis_time = datetime.now()
            self.data_version += 1
            
            logger.info("âœ… æ¨¡æ‹Ÿåˆ†æç»“æœç”Ÿæˆå®Œæˆ")
            return mock_results
            
        except Exception as e:
            logger.error(f"âŒ ç”Ÿæˆæ¨¡æ‹Ÿåˆ†æç»“æœå¤±è´¥: {str(e)}")
            return {}
    
    def _save_analysis_results(self, results: Dict[str, Any]):
        """ä¿å­˜åˆ†æç»“æœ"""
        try:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f'dynamic_analysis_{timestamp}.json'
            filepath = os.path.join(DATA_PATHS['ANALYSIS_RESULTS'], 'dynamic', filename)
            
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            
            # ä¿å­˜æœ€æ–°ç»“æœ
            latest_filepath = os.path.join(DATA_PATHS['ANALYSIS_RESULTS'], 'dynamic', 'latest_analysis.json')
            with open(latest_filepath, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            
            logger.info(f"âœ… åˆ†æç»“æœä¿å­˜æˆåŠŸ: {filename}")
            
        except Exception as e:
            logger.error(f"âŒ åˆ†æç»“æœä¿å­˜å¤±è´¥: {str(e)}")
    
    def get_latest_analysis(self) -> Dict[str, Any]:
        """è·å–æœ€æ–°åˆ†æç»“æœ"""
        try:
            if self.analysis_cache and self.last_analysis_time:
                # æ£€æŸ¥ç¼“å­˜æ˜¯å¦è¿‡æœŸï¼ˆ1å°æ—¶ï¼‰
                if datetime.now() - self.last_analysis_time < timedelta(hours=1):
                    return self.analysis_cache
            
            # ä»æ–‡ä»¶åŠ è½½æœ€æ–°ç»“æœ
            latest_filepath = os.path.join(DATA_PATHS['ANALYSIS_RESULTS'], 'dynamic', 'latest_analysis.json')
            if os.path.exists(latest_filepath):
                with open(latest_filepath, 'r', encoding='utf-8') as f:
                    results = json.load(f)
                    self.analysis_cache = results
                    return results
            
            return {}
            
        except Exception as e:
            logger.error(f"âŒ è·å–æœ€æ–°åˆ†æç»“æœå¤±è´¥: {str(e)}")
            return {}
    
    def trigger_analysis_on_data_import(self, import_source: str, data_count: int):
        """åœ¨æ•°æ®å¯¼å…¥æ—¶è§¦å‘åˆ†æ"""
        try:
            logger.info(f"ğŸ“Š æ•°æ®å¯¼å…¥è§¦å‘åˆ†æ: {import_source}, æ•°æ®é‡: {data_count}")
            
            # è¿è¡ŒåŠ¨æ€åˆ†æ
            results = self.run_dynamic_analysis(trigger_source=f"data_import_{import_source}")
            
            # è®°å½•å¯¼å…¥äº‹ä»¶
            import_log = {
                'timestamp': datetime.now().isoformat(),
                'source': import_source,
                'data_count': data_count,
                'analysis_triggered': True,
                'analysis_results_available': bool(results)
            }
            
            # ä¿å­˜å¯¼å…¥æ—¥å¿—
            log_filepath = os.path.join(DATA_PATHS['ANALYSIS_RESULTS'], 'dynamic', 'import_log.json')
            if os.path.exists(log_filepath):
                with open(log_filepath, 'r', encoding='utf-8') as f:
                    logs = json.load(f)
            else:
                logs = []
            
            logs.append(import_log)
            
            with open(log_filepath, 'w', encoding='utf-8') as f:
                json.dump(logs, f, ensure_ascii=False, indent=2)
            
            logger.info("âœ… æ•°æ®å¯¼å…¥åˆ†æè§¦å‘å®Œæˆ")
            return results
            
        except Exception as e:
            logger.error(f"âŒ æ•°æ®å¯¼å…¥åˆ†æè§¦å‘å¤±è´¥: {str(e)}")
            return {}

def main():
    """ä¸»å‡½æ•°"""
    analyzer = DynamicAnalyzer()
    
    # è¿è¡ŒåŠ¨æ€åˆ†æ
    results = analyzer.run_dynamic_analysis()
    
    if results:
        print("âœ… åŠ¨æ€åˆ†æå®Œæˆ")
        print(f"åˆ†ææ—¶é—´: {results.get('analysis_timestamp')}")
        print(f"æ•°æ®æº: {results.get('data_sources')}")
        print(f"æ•°æ®é‡: {results.get('data_counts')}")
    else:
        print("âŒ åŠ¨æ€åˆ†æå¤±è´¥")

if __name__ == "__main__":
    main()
