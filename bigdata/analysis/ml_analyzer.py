"""
æŠ¤å·¥èµ„æºç®¡ç†ç³»ç»Ÿ - æœºå™¨å­¦ä¹ åˆ†æ
====================================

ä½¿ç”¨æœºå™¨å­¦ä¹ ç®—æ³•è¿›è¡Œæ•°æ®åˆ†æå’Œé¢„æµ‹
"""

import findspark
findspark.init()

from pyspark.sql import SparkSession
from pyspark.ml.classification import RandomForestClassifier, LogisticRegression
from pyspark.ml.regression import RandomForestRegressor, LinearRegression
from pyspark.ml.clustering import KMeans
from pyspark.ml.recommendation import ALS
from pyspark.ml.evaluation import MulticlassClassificationEvaluator, RegressionEvaluator
from pyspark.ml.tuning import CrossValidator, ParamGridBuilder
from pyspark.sql.functions import *
import numpy as np
import pandas as pd
import logging
from bigdata_config import SPARK_CONFIG, ML_CONFIG, DATA_PATHS

logger = logging.getLogger(__name__)

class MLAnalyzer:
    """æœºå™¨å­¦ä¹ åˆ†æç±»"""
    
    def __init__(self):
        self.spark = None
        self.initialize_spark()
    
    def initialize_spark(self):
        """åˆå§‹åŒ–Sparkä¼šè¯"""
        try:
            self.spark = SparkSession.builder \
                .appName("CaregiverMLAnalysis") \
                .master(SPARK_CONFIG['SPARK_MASTER']) \
                .config("spark.driver.memory", "4g") \
                .config("spark.executor.memory", "2g") \
                .getOrCreate()
            
            logger.info("âœ… Spark MLä¼šè¯åˆå§‹åŒ–æˆåŠŸ")
            
        except Exception as e:
            logger.error(f"âŒ Spark MLåˆå§‹åŒ–å¤±è´¥: {str(e)}")
            raise
    
    def load_processed_data(self):
        """åŠ è½½å¤„ç†åçš„æ•°æ®"""
        try:
            # åŠ è½½æŠ¤å·¥ç‰¹å¾æ•°æ®
            caregiver_df = self.spark.read.parquet(f"{DATA_PATHS['PROCESSED_DATA']}/caregiver_features")
            
            # åŠ è½½çˆ¬è™«æ•°æ®
            crawled_df = self.spark.read.parquet(f"{DATA_PATHS['PROCESSED_DATA']}/crawled_data_clean")
            
            logger.info("âœ… å¤„ç†åæ•°æ®åŠ è½½æˆåŠŸ")
            return caregiver_df, crawled_df
            
        except Exception as e:
            logger.error(f"âŒ å¤„ç†åæ•°æ®åŠ è½½å¤±è´¥: {str(e)}")
            return None, None
    
    def predict_success_rate(self, df):
        """é¢„æµ‹æŠ¤å·¥æˆåŠŸç‡"""
        try:
            # å‡†å¤‡è®­ç»ƒæ•°æ®
            # å‡è®¾æˆ‘ä»¬æœ‰å†å²æˆåŠŸç‡æ•°æ®
            training_df = df.withColumn(
                "success_rate",
                when(col("rating_clean") >= 4.0, 1.0)
                .when(col("rating_clean") >= 3.0, 0.7)
                .when(col("rating_clean") >= 2.0, 0.4)
                .otherwise(0.1)
            )
            
            # åˆ†å‰²è®­ç»ƒå’Œæµ‹è¯•æ•°æ®
            train_data, test_data = training_df.randomSplit([0.8, 0.2], seed=42)
            
            # åˆ›å»ºéšæœºæ£®æ—å›å½’å™¨
            rf = RandomForestRegressor(
                featuresCol="features",
                labelCol="success_rate",
                numTrees=100,
                maxDepth=10,
                seed=42
            )
            
            # è®­ç»ƒæ¨¡å‹
            model = rf.fit(train_data)
            
            # é¢„æµ‹
            predictions = model.transform(test_data)
            
            # è¯„ä¼°æ¨¡å‹
            evaluator = RegressionEvaluator(
                labelCol="success_rate",
                predictionCol="prediction",
                metricName="rmse"
            )
            
            rmse = evaluator.evaluate(predictions)
            logger.info(f"âœ… æˆåŠŸç‡é¢„æµ‹æ¨¡å‹RMSE: {rmse}")
            
            # ä¿å­˜æ¨¡å‹
            model.write().overwrite().save(f"{ML_CONFIG['MODEL_SAVE_PATH']}/success_rate_model")
            
            return model, predictions
            
        except Exception as e:
            logger.error(f"âŒ æˆåŠŸç‡é¢„æµ‹å¤±è´¥: {str(e)}")
            return None, None
    
    def cluster_caregivers(self, df):
        """æŠ¤å·¥èšç±»åˆ†æ"""
        try:
            # ä½¿ç”¨K-meansèšç±»
            kmeans = KMeans(
                featuresCol="features",
                k=5,  # 5ä¸ªèšç±»
                seed=42
            )
            
            # è®­ç»ƒæ¨¡å‹
            model = kmeans.fit(df)
            
            # é¢„æµ‹èšç±»
            predictions = model.transform(df)
            
            # åˆ†æèšç±»ç»“æœ
            cluster_stats = predictions.groupBy("prediction").agg(
                count("*").alias("count"),
                avg("hourly_rate_clean").alias("avg_hourly_rate"),
                avg("rating_clean").alias("avg_rating"),
                avg("age_clean").alias("avg_age")
            ).orderBy("prediction")
            
            logger.info("âœ… æŠ¤å·¥èšç±»åˆ†æå®Œæˆ")
            cluster_stats.show()
            
            # ä¿å­˜æ¨¡å‹
            model.write().overwrite().save(f"{ML_CONFIG['MODEL_SAVE_PATH']}/caregiver_cluster_model")
            
            return model, predictions
            
        except Exception as e:
            logger.error(f"âŒ æŠ¤å·¥èšç±»åˆ†æå¤±è´¥: {str(e)}")
            return None, None
    
    def recommend_caregivers(self, user_df, caregiver_df, appointment_df):
        """æŠ¤å·¥æ¨èç³»ç»Ÿ"""
        try:
            # å‡†å¤‡æ¨èæ•°æ®
            # åŸºäºç”¨æˆ·å¯¹æŠ¤å·¥çš„è¯„ä»·åˆ›å»ºæ¨èæ•°æ®
            recommendation_data = appointment_df.join(
                user_df, appointment_df.user_id == user_df.id
            ).join(
                caregiver_df, appointment_df.caregiver_id == caregiver_df.id
            ).select(
                col("user_id").cast("int"),
                col("caregiver_id").cast("int"),
                col("rating_clean").alias("rating")
            ).filter(col("rating").isNotNull())
            
            # åˆ†å‰²è®­ç»ƒå’Œæµ‹è¯•æ•°æ®
            train_data, test_data = recommendation_data.randomSplit([0.8, 0.2], seed=42)
            
            # åˆ›å»ºALSæ¨èæ¨¡å‹
            als = ALS(
                maxIter=10,
                regParam=0.01,
                userCol="user_id",
                itemCol="caregiver_id",
                ratingCol="rating",
                coldStartStrategy="drop"
            )
            
            # è®­ç»ƒæ¨¡å‹
            model = als.fit(train_data)
            
            # é¢„æµ‹
            predictions = model.transform(test_data)
            
            # è¯„ä¼°æ¨¡å‹
            evaluator = RegressionEvaluator(
                metricName="rmse",
                labelCol="rating",
                predictionCol="prediction"
            )
            
            rmse = evaluator.evaluate(predictions)
            logger.info(f"âœ… æ¨èç³»ç»ŸRMSE: {rmse}")
            
            # ä¿å­˜æ¨¡å‹
            model.write().overwrite().save(f"{ML_CONFIG['MODEL_SAVE_PATH']}/recommendation_model")
            
            return model, predictions
            
        except Exception as e:
            logger.error(f"âŒ æ¨èç³»ç»Ÿè®­ç»ƒå¤±è´¥: {str(e)}")
            return None, None
    
    def predict_demand(self, df):
        """é¢„æµ‹æŠ¤å·¥éœ€æ±‚"""
        try:
            # åŸºäºå†å²æ•°æ®é¢„æµ‹éœ€æ±‚
            # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…åº”è¯¥åŸºäºæ—¶é—´åºåˆ—æ•°æ®
            
            # åˆ›å»ºéœ€æ±‚é¢„æµ‹ç‰¹å¾
            demand_df = df.groupBy("city", "service_type").agg(
                count("*").alias("demand_count"),
                avg("hourly_rate_clean").alias("avg_rate"),
                avg("rating_clean").alias("avg_rating")
            )
            
            # ä½¿ç”¨çº¿æ€§å›å½’é¢„æµ‹éœ€æ±‚
            from pyspark.ml.feature import VectorAssembler
            
            assembler = VectorAssembler(
                inputCols=["avg_rate", "avg_rating"],
                outputCol="features"
            )
            
            demand_features = assembler.transform(demand_df)
            
            # åˆ›å»ºçº¿æ€§å›å½’æ¨¡å‹
            lr = LinearRegression(
                featuresCol="features",
                labelCol="demand_count"
            )
            
            # è®­ç»ƒæ¨¡å‹
            model = lr.fit(demand_features)
            
            # é¢„æµ‹
            predictions = model.transform(demand_features)
            
            # è¯„ä¼°æ¨¡å‹
            evaluator = RegressionEvaluator(
                labelCol="demand_count",
                predictionCol="prediction",
                metricName="rmse"
            )
            
            rmse = evaluator.evaluate(predictions)
            logger.info(f"âœ… éœ€æ±‚é¢„æµ‹æ¨¡å‹RMSE: {rmse}")
            
            # ä¿å­˜æ¨¡å‹
            model.write().overwrite().save(f"{ML_CONFIG['MODEL_SAVE_PATH']}/demand_prediction_model")
            
            return model, predictions
            
        except Exception as e:
            logger.error(f"âŒ éœ€æ±‚é¢„æµ‹å¤±è´¥: {str(e)}")
            return None, None
    
    def analyze_market_trends(self, crawled_df):
        """åˆ†æå¸‚åœºè¶‹åŠ¿"""
        try:
            # åˆ†æè–ªèµ„è¶‹åŠ¿
            salary_trend = crawled_df.groupBy("city", "service_type").agg(
                avg("salary_avg_clean").alias("avg_salary"),
                count("*").alias("job_count"),
                min("salary_avg_clean").alias("min_salary"),
                max("salary_avg_clean").alias("max_salary")
            ).orderBy("avg_salary", ascending=False)
            
            # åˆ†æéœ€æ±‚è¶‹åŠ¿
            demand_trend = crawled_df.groupBy("city").agg(
                count("*").alias("total_jobs"),
                countDistinct("service_type").alias("service_types"),
                avg("salary_avg_clean").alias("avg_salary")
            ).orderBy("total_jobs", ascending=False)
            
            logger.info("âœ… å¸‚åœºè¶‹åŠ¿åˆ†æå®Œæˆ")
            
            # ä¿å­˜åˆ†æç»“æœ
            salary_trend.write.mode("overwrite").parquet(f"{DATA_PATHS['ANALYSIS_RESULTS']}/salary_trend")
            demand_trend.write.mode("overwrite").parquet(f"{DATA_PATHS['ANALYSIS_RESULTS']}/demand_trend")
            
            return salary_trend, demand_trend
            
        except Exception as e:
            logger.error(f"âŒ å¸‚åœºè¶‹åŠ¿åˆ†æå¤±è´¥: {str(e)}")
            return None, None
    
    def run_all_analysis(self):
        """è¿è¡Œæ‰€æœ‰åˆ†æ"""
        try:
            logger.info("ğŸš€ å¼€å§‹æœºå™¨å­¦ä¹ åˆ†æ")
            
            # åŠ è½½æ•°æ®
            caregiver_df, crawled_df = self.load_processed_data()
            
            if caregiver_df is None:
                logger.error("âŒ æ— æ³•åŠ è½½æŠ¤å·¥æ•°æ®")
                return
            
            # 1. é¢„æµ‹æŠ¤å·¥æˆåŠŸç‡
            logger.info("ğŸ“Š å¼€å§‹æˆåŠŸç‡é¢„æµ‹åˆ†æ")
            success_model, success_predictions = self.predict_success_rate(caregiver_df)
            
            # 2. æŠ¤å·¥èšç±»åˆ†æ
            logger.info("ğŸ“Š å¼€å§‹æŠ¤å·¥èšç±»åˆ†æ")
            cluster_model, cluster_predictions = self.cluster_caregivers(caregiver_df)
            
            # 3. å¸‚åœºè¶‹åŠ¿åˆ†æ
            if crawled_df is not None:
                logger.info("ğŸ“Š å¼€å§‹å¸‚åœºè¶‹åŠ¿åˆ†æ")
                salary_trend, demand_trend = self.analyze_market_trends(crawled_df)
            
            # 4. éœ€æ±‚é¢„æµ‹
            logger.info("ğŸ“Š å¼€å§‹éœ€æ±‚é¢„æµ‹åˆ†æ")
            demand_model, demand_predictions = self.predict_demand(caregiver_df)
            
            logger.info("âœ… æœºå™¨å­¦ä¹ åˆ†æå®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ æœºå™¨å­¦ä¹ åˆ†æå¤±è´¥: {str(e)}")
    
    def close(self):
        """å…³é—­Sparkä¼šè¯"""
        if self.spark:
            self.spark.stop()
            logger.info("âœ… Spark MLä¼šè¯å·²å…³é—­")

def main():
    """ä¸»å‡½æ•°"""
    analyzer = MLAnalyzer()
    try:
        analyzer.run_all_analysis()
    finally:
        analyzer.close()

if __name__ == "__main__":
    main()
