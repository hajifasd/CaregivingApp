"""
æŠ¤å·¥èµ„æºç®¡ç†ç³»ç»Ÿ - åŠ¨æ€åˆ†æç¤ºä¾‹
====================================

æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨åŠ¨æ€åˆ†æåŠŸèƒ½
"""

import os
import sys
import pandas as pd
import json
from datetime import datetime

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))

from bigdata.processing.data_import_trigger import DataImportTrigger
from bigdata.analysis.dynamic_analyzer import DynamicAnalyzer

def create_sample_data():
    """åˆ›å»ºç¤ºä¾‹æ•°æ®"""
    print("ğŸ“Š åˆ›å»ºç¤ºä¾‹æ•°æ®...")
    
    # åˆ›å»ºæŠ¤å·¥æ•°æ®
    caregiver_data = {
        'name': ['å¼ æŠ¤å·¥', 'ææŠ¤å·¥', 'ç‹æŠ¤å·¥', 'èµµæŠ¤å·¥', 'é™ˆæŠ¤å·¥'],
        'age': [35, 42, 28, 45, 38],
        'gender': ['å¥³', 'ç”·', 'å¥³', 'ç”·', 'å¥³'],
        'city': ['åŒ—äº¬', 'ä¸Šæµ·', 'å¹¿å·', 'æ·±åœ³', 'åŒ—äº¬'],
        'hourly_rate': [50, 60, 45, 55, 48],
        'rating': [4.5, 4.8, 4.2, 4.6, 4.3],
        'service_type': ['å…»è€æŠ¤ç†', 'åº·å¤æŠ¤ç†', 'åŒ»ç–—æŠ¤ç†', 'å…»è€æŠ¤ç†', 'åº·å¤æŠ¤ç†']
    }
    
    caregiver_df = pd.DataFrame(caregiver_data)
    caregiver_file = 'sample_caregiver_data.csv'
    caregiver_df.to_csv(caregiver_file, index=False, encoding='utf-8')
    print(f"âœ… æŠ¤å·¥æ•°æ®åˆ›å»ºå®Œæˆ: {caregiver_file}")
    
    # åˆ›å»ºæ‹›è˜æ•°æ®
    job_data = {
        'title': ['æŠ¤å·¥æ‹›è˜', 'æŠ¤ç†å‘˜æ‹›è˜', 'åº·å¤æŠ¤ç†æ‹›è˜', 'åŒ»ç–—æŠ¤ç†æ‹›è˜', 'å…»è€æŠ¤ç†æ‹›è˜'],
        'company': ['åŒ—äº¬å…»è€é™¢', 'ä¸Šæµ·åº·å¤ä¸­å¿ƒ', 'å¹¿å·åŒ»é™¢', 'æ·±åœ³æŠ¤ç†ä¸­å¿ƒ', 'åŒ—äº¬åŒ»ç–—é›†å›¢'],
        'location': ['åŒ—äº¬æœé˜³åŒº', 'ä¸Šæµ·æµ¦ä¸œåŒº', 'å¹¿å·å¤©æ²³åŒº', 'æ·±åœ³å—å±±åŒº', 'åŒ—äº¬æµ·æ·€åŒº'],
        'salary': ['5000-8000å…ƒ/æœˆ', '6000-9000å…ƒ/æœˆ', '4500-7000å…ƒ/æœˆ', '5500-8500å…ƒ/æœˆ', '4800-7500å…ƒ/æœˆ'],
        'requirements': ['æœ‰ç»éªŒä¼˜å…ˆ', 'æŠ¤ç†ä¸“ä¸š', 'åº·å¤ä¸“ä¸š', 'åŒ»ç–—ä¸“ä¸š', 'å…»è€æŠ¤ç†ç»éªŒ']
    }
    
    job_df = pd.DataFrame(job_data)
    job_file = 'sample_job_data.csv'
    job_df.to_csv(job_file, index=False, encoding='utf-8')
    print(f"âœ… æ‹›è˜æ•°æ®åˆ›å»ºå®Œæˆ: {job_file}")
    
    return caregiver_file, job_file

def demonstrate_dynamic_analysis():
    """æ¼”ç¤ºåŠ¨æ€åˆ†æåŠŸèƒ½"""
    print("\nğŸš€ å¼€å§‹åŠ¨æ€åˆ†ææ¼”ç¤º")
    
    # åˆ›å»ºç¤ºä¾‹æ•°æ®
    caregiver_file, job_file = create_sample_data()
    
    # åˆå§‹åŒ–æ•°æ®å¯¼å…¥è§¦å‘å™¨
    trigger = DataImportTrigger()
    
    # å¯¼å…¥æŠ¤å·¥æ•°æ®å¹¶è§¦å‘åˆ†æ
    print("\nğŸ“Š å¯¼å…¥æŠ¤å·¥æ•°æ®...")
    success1 = trigger.import_csv_data(caregiver_file, "sample_caregivers")
    if success1:
        print("âœ… æŠ¤å·¥æ•°æ®å¯¼å…¥æˆåŠŸï¼Œåˆ†æå·²è§¦å‘")
    else:
        print("âŒ æŠ¤å·¥æ•°æ®å¯¼å…¥å¤±è´¥")
    
    # å¯¼å…¥æ‹›è˜æ•°æ®å¹¶è§¦å‘åˆ†æ
    print("\nğŸ“Š å¯¼å…¥æ‹›è˜æ•°æ®...")
    success2 = trigger.import_csv_data(job_file, "sample_jobs")
    if success2:
        print("âœ… æ‹›è˜æ•°æ®å¯¼å…¥æˆåŠŸï¼Œåˆ†æå·²è§¦å‘")
    else:
        print("âŒ æ‹›è˜æ•°æ®å¯¼å…¥å¤±è´¥")
    
    # ç›´æ¥è¿è¡ŒåŠ¨æ€åˆ†æ
    print("\nğŸ¤– ç›´æ¥è¿è¡ŒåŠ¨æ€åˆ†æ...")
    analyzer = DynamicAnalyzer()
    results = analyzer.run_dynamic_analysis(trigger_source="demo")
    
    if results:
        print("âœ… åŠ¨æ€åˆ†æå®Œæˆ")
        print(f"åˆ†ææ—¶é—´: {results.get('analysis_timestamp')}")
        print(f"æ•°æ®ç‰ˆæœ¬: {results.get('data_version')}")
        print(f"æ•°æ®æº: {results.get('data_sources')}")
        print(f"æ•°æ®é‡: {results.get('data_counts')}")
        
        # æ˜¾ç¤ºåˆ†æç»“æœ
        if 'caregiver_distribution' in results:
            dist = results['caregiver_distribution']
            print(f"\nğŸ“ˆ æŠ¤å·¥åˆ†å¸ƒåˆ†æ:")
            print(f"  æ€»æŠ¤å·¥æ•°: {dist.get('total_caregivers', 0)}")
            print(f"  åœ°åŸŸåˆ†å¸ƒ: {dist.get('location_distribution', {})}")
            print(f"  è–ªèµ„åˆ†å¸ƒ: {dist.get('salary_distribution', {})}")
        
        if 'success_prediction' in results:
            pred = results['success_prediction']
            print(f"\nğŸ¯ æˆåŠŸç‡é¢„æµ‹:")
            print(f"  æ¨¡å‹å‡†ç¡®ç‡: {pred.get('model_accuracy', 0):.3f}")
            print(f"  ç‰¹å¾é‡è¦æ€§: {pred.get('feature_importance', {})}")
        
        if 'cluster_analysis' in results:
            cluster = results['cluster_analysis']
            print(f"\nğŸ” èšç±»åˆ†æ:")
            print(f"  èšç±»æ•°é‡: {cluster.get('total_clusters', 0)}")
            for cluster_info in cluster.get('clusters', []):
                print(f"  èšç±» {cluster_info['cluster_id']}: {cluster_info['count']} ä¸ªæŠ¤å·¥")
    else:
        print("âŒ åŠ¨æ€åˆ†æå¤±è´¥")
    
    # æ˜¾ç¤ºå¯¼å…¥å†å²
    print("\nğŸ“‹ å¯¼å…¥å†å²:")
    history = trigger.get_import_history()
    for log in history:
        print(f"  {log['timestamp']}: {log['source']} ({log['count']} æ¡è®°å½•)")
    
    # æ¸…ç†ç¤ºä¾‹æ–‡ä»¶
    print("\nğŸ§¹ æ¸…ç†ç¤ºä¾‹æ–‡ä»¶...")
    try:
        os.remove(caregiver_file)
        os.remove(job_file)
        print("âœ… ç¤ºä¾‹æ–‡ä»¶æ¸…ç†å®Œæˆ")
    except:
        print("âš ï¸ ç¤ºä¾‹æ–‡ä»¶æ¸…ç†å¤±è´¥")

def demonstrate_api_usage():
    """æ¼”ç¤ºAPIä½¿ç”¨"""
    print("\nğŸŒ APIä½¿ç”¨æ¼”ç¤º")
    
    import requests
    
    try:
        # è§¦å‘åŠ¨æ€åˆ†æ
        print("ğŸ“¡ é€šè¿‡APIè§¦å‘åŠ¨æ€åˆ†æ...")
        response = requests.post('http://localhost:8000/api/bigdata/dynamic/trigger', 
                               json={'source': 'api_demo'}, timeout=10)
        
        if response.status_code == 200:
            result = response.json()
            if result.get('success'):
                print("âœ… APIè§¦å‘æˆåŠŸ")
                print(f"åˆ†ææ—¶é—´: {result['data']['analysis_timestamp']}")
            else:
                print(f"âŒ APIè§¦å‘å¤±è´¥: {result.get('message')}")
        else:
            print(f"âŒ APIè°ƒç”¨å¤±è´¥: {response.status_code}")
        
        # è·å–æœ€æ–°åˆ†æç»“æœ
        print("\nğŸ“¡ é€šè¿‡APIè·å–æœ€æ–°åˆ†æç»“æœ...")
        response = requests.get('http://localhost:8000/api/bigdata/dynamic/latest', timeout=10)
        
        if response.status_code == 200:
            result = response.json()
            if result.get('success'):
                print("âœ… è·å–æœ€æ–°åˆ†ææˆåŠŸ")
                data = result['data']
                print(f"æ•°æ®ç‰ˆæœ¬: {data.get('data_version')}")
                print(f"æ•°æ®æº: {data.get('data_sources')}")
            else:
                print(f"âš ï¸ æ²¡æœ‰å¯ç”¨åˆ†æç»“æœ: {result.get('message')}")
        else:
            print(f"âŒ APIè°ƒç”¨å¤±è´¥: {response.status_code}")
            
    except requests.exceptions.ConnectionError:
        print("âŒ æ— æ³•è¿æ¥åˆ°APIæœåŠ¡å™¨ï¼Œè¯·ç¡®ä¿Flaskåº”ç”¨æ­£åœ¨è¿è¡Œ")
    except Exception as e:
        print(f"âŒ APIè°ƒç”¨å¼‚å¸¸: {str(e)}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ æŠ¤å·¥èµ„æºç®¡ç†ç³»ç»Ÿ - åŠ¨æ€åˆ†ææ¼”ç¤º")
    print("=" * 50)
    
    # æ¼”ç¤ºåŠ¨æ€åˆ†æåŠŸèƒ½
    demonstrate_dynamic_analysis()
    
    # æ¼”ç¤ºAPIä½¿ç”¨
    demonstrate_api_usage()
    
    print("\nğŸ‰ æ¼”ç¤ºå®Œæˆï¼")
    print("\nğŸ’¡ ä½¿ç”¨è¯´æ˜:")
    print("1. æ¯æ¬¡å¯¼å…¥æ–°æ•°æ®æ—¶ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨è§¦å‘åŠ¨æ€åˆ†æ")
    print("2. å¯ä»¥é€šè¿‡APIæ‰‹åŠ¨è§¦å‘åˆ†æ")
    print("3. åˆ†æç»“æœä¼šå®æ—¶æ›´æ–°åˆ°ä»ªè¡¨ç›˜")
    print("4. æ”¯æŒå¤šç§æ•°æ®æ ¼å¼ï¼šCSVã€JSONã€Parquet")
    print("5. åˆ†æç»“æœä¼šä¿å­˜åˆ°æ–‡ä»¶ï¼Œæ”¯æŒå†å²æŸ¥è¯¢")

if __name__ == "__main__":
    main()

