"""
æŠ¤å·¥èµ„æºç®¡ç†ç³»ç»Ÿ - æ•°æ®å¯¼å…¥è§¦å‘å™¨
====================================

å½“æœ‰æ–°æ•°æ®å¯¼å…¥æ—¶è‡ªåŠ¨è§¦å‘åŠ¨æ€åˆ†æ
"""

import os
import json
import logging
import requests
from datetime import datetime
from typing import Dict, Any, Optional
import pandas as pd
from bigdata_config import DATA_PATHS

logger = logging.getLogger(__name__)

class DataImportTrigger:
    """æ•°æ®å¯¼å…¥è§¦å‘å™¨"""
    
    def __init__(self, api_base_url: str = "http://localhost:8000"):
        self.api_base_url = api_base_url
        self.import_log_file = os.path.join(DATA_PATHS['ANALYSIS_RESULTS'], 'dynamic', 'import_log.json')
        
        # åˆ›å»ºå¿…è¦çš„ç›®å½•
        os.makedirs(os.path.dirname(self.import_log_file), exist_ok=True)
    
    def trigger_analysis(self, source: str, data_count: int, data_type: str = "csv") -> bool:
        """è§¦å‘åŠ¨æ€åˆ†æ"""
        try:
            # è°ƒç”¨APIè§¦å‘åˆ†æ
            url = f"{self.api_base_url}/api/bigdata/dynamic/import-trigger"
            payload = {
                'source': source,
                'count': data_count,
                'type': data_type,
                'timestamp': datetime.now().isoformat()
            }
            
            response = requests.post(url, json=payload, timeout=30)
            
            if response.status_code == 200:
                result = response.json()
                if result.get('success'):
                    logger.info(f"âœ… æ•°æ®å¯¼å…¥åˆ†æè§¦å‘æˆåŠŸ: {source}, æ•°æ®é‡: {data_count}")
                    return True
                else:
                    logger.error(f"âŒ æ•°æ®å¯¼å…¥åˆ†æè§¦å‘å¤±è´¥: {result.get('message')}")
                    return False
            else:
                logger.error(f"âŒ APIè°ƒç”¨å¤±è´¥: {response.status_code}")
                return False
                
        except Exception as e:
            logger.error(f"âŒ è§¦å‘åˆ†æå¤±è´¥: {str(e)}")
            return False
    
    def import_csv_data(self, file_path: str, source_name: str = None) -> bool:
        """å¯¼å…¥CSVæ•°æ®å¹¶è§¦å‘åˆ†æ"""
        try:
            if not os.path.exists(file_path):
                logger.error(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
                return False
            
            # è¯»å–CSVæ–‡ä»¶
            df = pd.read_csv(file_path)
            data_count = len(df)
            
            if source_name is None:
                source_name = os.path.basename(file_path).replace('.csv', '')
            
            logger.info(f"ğŸ“Š å¯¼å…¥CSVæ•°æ®: {source_name}, æ•°æ®é‡: {data_count}")
            
            # ä¿å­˜åˆ°å¤„ç†åçš„æ•°æ®ç›®å½•
            processed_file = os.path.join(DATA_PATHS['PROCESSED_DATA'], f"{source_name}_processed.parquet")
            df.to_parquet(processed_file, index=False)
            
            # è§¦å‘åˆ†æ
            success = self.trigger_analysis(source_name, data_count, "csv")
            
            if success:
                # è®°å½•å¯¼å…¥æ—¥å¿—
                self._log_import(source_name, data_count, "csv", file_path)
            
            return success
            
        except Exception as e:
            logger.error(f"âŒ CSVæ•°æ®å¯¼å…¥å¤±è´¥: {str(e)}")
            return False
    
    def import_json_data(self, file_path: str, source_name: str = None) -> bool:
        """å¯¼å…¥JSONæ•°æ®å¹¶è§¦å‘åˆ†æ"""
        try:
            if not os.path.exists(file_path):
                logger.error(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
                return False
            
            # è¯»å–JSONæ–‡ä»¶
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            if isinstance(data, list):
                data_count = len(data)
                df = pd.DataFrame(data)
            else:
                data_count = 1
                df = pd.DataFrame([data])
            
            if source_name is None:
                source_name = os.path.basename(file_path).replace('.json', '')
            
            logger.info(f"ğŸ“Š å¯¼å…¥JSONæ•°æ®: {source_name}, æ•°æ®é‡: {data_count}")
            
            # ä¿å­˜åˆ°å¤„ç†åçš„æ•°æ®ç›®å½•
            processed_file = os.path.join(DATA_PATHS['PROCESSED_DATA'], f"{source_name}_processed.parquet")
            df.to_parquet(processed_file, index=False)
            
            # è§¦å‘åˆ†æ
            success = self.trigger_analysis(source_name, data_count, "json")
            
            if success:
                # è®°å½•å¯¼å…¥æ—¥å¿—
                self._log_import(source_name, data_count, "json", file_path)
            
            return success
            
        except Exception as e:
            logger.error(f"âŒ JSONæ•°æ®å¯¼å…¥å¤±è´¥: {str(e)}")
            return False
    
    def import_parquet_data(self, file_path: str, source_name: str = None) -> bool:
        """å¯¼å…¥Parquetæ•°æ®å¹¶è§¦å‘åˆ†æ"""
        try:
            if not os.path.exists(file_path):
                logger.error(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
                return False
            
            # è¯»å–Parquetæ–‡ä»¶
            df = pd.read_parquet(file_path)
            data_count = len(df)
            
            if source_name is None:
                source_name = os.path.basename(file_path).replace('.parquet', '')
            
            logger.info(f"ğŸ“Š å¯¼å…¥Parquetæ•°æ®: {source_name}, æ•°æ®é‡: {data_count}")
            
            # ä¿å­˜åˆ°å¤„ç†åçš„æ•°æ®ç›®å½•
            processed_file = os.path.join(DATA_PATHS['PROCESSED_DATA'], f"{source_name}_processed.parquet")
            df.to_parquet(processed_file, index=False)
            
            # è§¦å‘åˆ†æ
            success = self.trigger_analysis(source_name, data_count, "parquet")
            
            if success:
                # è®°å½•å¯¼å…¥æ—¥å¿—
                self._log_import(source_name, data_count, "parquet", file_path)
            
            return success
            
        except Exception as e:
            logger.error(f"âŒ Parquetæ•°æ®å¯¼å…¥å¤±è´¥: {str(e)}")
            return False
    
    def _log_import(self, source: str, count: int, data_type: str, file_path: str):
        """è®°å½•å¯¼å…¥æ—¥å¿—"""
        try:
            log_entry = {
                'timestamp': datetime.now().isoformat(),
                'source': source,
                'count': count,
                'type': data_type,
                'file_path': file_path,
                'status': 'success'
            }
            
            # è¯»å–ç°æœ‰æ—¥å¿—
            if os.path.exists(self.import_log_file):
                with open(self.import_log_file, 'r', encoding='utf-8') as f:
                    logs = json.load(f)
            else:
                logs = []
            
            # æ·»åŠ æ–°æ—¥å¿—
            logs.append(log_entry)
            
            # ä¿å­˜æ—¥å¿—
            with open(self.import_log_file, 'w', encoding='utf-8') as f:
                json.dump(logs, f, ensure_ascii=False, indent=2)
            
            logger.info(f"âœ… å¯¼å…¥æ—¥å¿—è®°å½•æˆåŠŸ: {source}")
            
        except Exception as e:
            logger.error(f"âŒ å¯¼å…¥æ—¥å¿—è®°å½•å¤±è´¥: {str(e)}")
    
    def get_import_history(self) -> list:
        """è·å–å¯¼å…¥å†å²"""
        try:
            if os.path.exists(self.import_log_file):
                with open(self.import_log_file, 'r', encoding='utf-8') as f:
                    logs = json.load(f)
                return logs
            else:
                return []
                
        except Exception as e:
            logger.error(f"âŒ è·å–å¯¼å…¥å†å²å¤±è´¥: {str(e)}")
            return []
    
    def monitor_data_directory(self, directory: str, file_patterns: list = None):
        """ç›‘æ§æ•°æ®ç›®å½•ï¼Œè‡ªåŠ¨å¯¼å…¥æ–°æ–‡ä»¶"""
        try:
            if file_patterns is None:
                file_patterns = ['*.csv', '*.json', '*.parquet']
            
            if not os.path.exists(directory):
                logger.warning(f"âš ï¸ ç›‘æ§ç›®å½•ä¸å­˜åœ¨: {directory}")
                return
            
            # è·å–ç›®å½•ä¸­çš„æ–‡ä»¶
            files = []
            for pattern in file_patterns:
                import glob
                files.extend(glob.glob(os.path.join(directory, pattern)))
            
            # æ£€æŸ¥æ¯ä¸ªæ–‡ä»¶æ˜¯å¦éœ€è¦å¯¼å…¥
            for file_path in files:
                file_name = os.path.basename(file_path)
                
                # æ£€æŸ¥æ˜¯å¦å·²ç»å¯¼å…¥è¿‡
                history = self.get_import_history()
                already_imported = any(
                    log.get('file_path') == file_path and 
                    log.get('status') == 'success'
                    for log in history
                )
                
                if not already_imported:
                    # æ ¹æ®æ–‡ä»¶ç±»å‹å¯¼å…¥
                    if file_name.endswith('.csv'):
                        self.import_csv_data(file_path)
                    elif file_name.endswith('.json'):
                        self.import_json_data(file_path)
                    elif file_name.endswith('.parquet'):
                        self.import_parquet_data(file_path)
            
            logger.info(f"âœ… ç›®å½•ç›‘æ§å®Œæˆ: {directory}")
            
        except Exception as e:
            logger.error(f"âŒ ç›®å½•ç›‘æ§å¤±è´¥: {str(e)}")

def main():
    """ä¸»å‡½æ•° - ç¤ºä¾‹ç”¨æ³•"""
    trigger = DataImportTrigger()
    
    # ç¤ºä¾‹1: å¯¼å…¥CSVæ–‡ä»¶
    csv_file = "caregiver_jobs_5000.csv"
    if os.path.exists(csv_file):
        trigger.import_csv_data(csv_file, "caregiver_jobs_5000")
    
    # ç¤ºä¾‹2: å¯¼å…¥JSONæ–‡ä»¶
    json_file = "data/raw/jobs_5000_20250919_180126.json"
    if os.path.exists(json_file):
        trigger.import_json_data(json_file, "jobs_5000_20250919_180126")
    
    # ç¤ºä¾‹3: ç›‘æ§æ•°æ®ç›®å½•
    data_dir = "data/raw"
    if os.path.exists(data_dir):
        trigger.monitor_data_directory(data_dir)
    
    # æ˜¾ç¤ºå¯¼å…¥å†å²
    history = trigger.get_import_history()
    print(f"å¯¼å…¥å†å²è®°å½•: {len(history)} æ¡")
    for log in history[-5:]:  # æ˜¾ç¤ºæœ€è¿‘5æ¡
        print(f"- {log['timestamp']}: {log['source']} ({log['count']} æ¡è®°å½•)")

if __name__ == "__main__":
    main()

