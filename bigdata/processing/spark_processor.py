"""
æŠ¤å·¥èµ„æºç®¡ç†ç³»ç»Ÿ - Sparkæ•°æ®å¤„ç†
====================================

ä½¿ç”¨Sparkè¿›è¡Œå¤§è§„æ¨¡æ•°æ®å¤„ç†å’Œåˆ†æ
"""

import findspark
findspark.init()

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *
from pyspark.ml.feature import VectorAssembler, StringIndexer, OneHotEncoder
from pyspark.ml import Pipeline
import logging
from bigdata_config import SPARK_CONFIG, DATA_PATHS

logger = logging.getLogger(__name__)

class SparkDataProcessor:
    """Sparkæ•°æ®å¤„ç†ç±»"""
    
    def __init__(self):
        self.spark = None
        self.initialize_spark()
    
    def initialize_spark(self):
        """åˆå§‹åŒ–Sparkä¼šè¯"""
        try:
            self.spark = SparkSession.builder \
                .appName(SPARK_CONFIG['SPARK_APP_NAME']) \
                .master(SPARK_CONFIG['SPARK_MASTER']) \
                .config("spark.driver.memory", SPARK_CONFIG['SPARK_DRIVER_MEMORY']) \
                .config("spark.executor.memory", SPARK_CONFIG['SPARK_EXECUTOR_MEMORY']) \
                .config("spark.executor.cores", SPARK_CONFIG['SPARK_EXECUTOR_CORES']) \
                .config("spark.sql.adaptive.enabled", SPARK_CONFIG['SPARK_SQL_ADAPTIVE_ENABLED']) \
                .config("spark.sql.adaptive.coalescePartitions.enabled", SPARK_CONFIG['SPARK_SQL_ADAPTIVE_COALESCE_PARTITIONS_ENABLED']) \
                .getOrCreate()
            
            logger.info("âœ… Sparkä¼šè¯åˆå§‹åŒ–æˆåŠŸ")
            
        except Exception as e:
            logger.error(f"âŒ Sparkåˆå§‹åŒ–å¤±è´¥: {str(e)}")
            raise
    
    def load_caregiver_data(self):
        """åŠ è½½æŠ¤å·¥æ•°æ®"""
        try:
            # ä»MySQLåŠ è½½æŠ¤å·¥æ•°æ®
            caregiver_df = self.spark.read \
                .format("jdbc") \
                .option("url", "jdbc:mysql://localhost:3306/caregiving_db") \
                .option("dbtable", "caregiver") \
                .option("user", "root") \
                .option("password", "20040924") \
                .load()
            
            logger.info(f"âœ… æŠ¤å·¥æ•°æ®åŠ è½½æˆåŠŸ: {caregiver_df.count()} æ¡è®°å½•")
            return caregiver_df
            
        except Exception as e:
            logger.error(f"âŒ æŠ¤å·¥æ•°æ®åŠ è½½å¤±è´¥: {str(e)}")
            return None
    
    def load_user_data(self):
        """åŠ è½½ç”¨æˆ·æ•°æ®"""
        try:
            user_df = self.spark.read \
                .format("jdbc") \
                .option("url", "jdbc:mysql://localhost:3306/caregiving_db") \
                .option("dbtable", "user") \
                .option("user", "root") \
                .option("password", "20040924") \
                .load()
            
            logger.info(f"âœ… ç”¨æˆ·æ•°æ®åŠ è½½æˆåŠŸ: {user_df.count()} æ¡è®°å½•")
            return user_df
            
        except Exception as e:
            logger.error(f"âŒ ç”¨æˆ·æ•°æ®åŠ è½½å¤±è´¥: {str(e)}")
            return None
    
    def load_appointment_data(self):
        """åŠ è½½é¢„çº¦æ•°æ®"""
        try:
            appointment_df = self.spark.read \
                .format("jdbc") \
                .option("url", "jdbc:mysql://localhost:3306/caregiving_db") \
                .option("dbtable", "appointment") \
                .option("user", "root") \
                .option("password", "20040924") \
                .load()
            
            logger.info(f"âœ… é¢„çº¦æ•°æ®åŠ è½½æˆåŠŸ: {appointment_df.count()} æ¡è®°å½•")
            return appointment_df
            
        except Exception as e:
            logger.error(f"âŒ é¢„çº¦æ•°æ®åŠ è½½å¤±è´¥: {str(e)}")
            return None
    
    def load_crawled_data(self):
        """åŠ è½½çˆ¬è™«æ•°æ®"""
        try:
            # ä»MongoDBåŠ è½½çˆ¬è™«æ•°æ®
            crawled_df = self.spark.read \
                .format("mongo") \
                .option("uri", "mongodb://localhost:27017/caregiver_analytics.caregiver_jobs") \
                .load()
            
            logger.info(f"âœ… çˆ¬è™«æ•°æ®åŠ è½½æˆåŠŸ: {crawled_df.count()} æ¡è®°å½•")
            return crawled_df
            
        except Exception as e:
            logger.error(f"âŒ çˆ¬è™«æ•°æ®åŠ è½½å¤±è´¥: {str(e)}")
            return None
    
    def clean_caregiver_data(self, df):
        """æ¸…æ´—æŠ¤å·¥æ•°æ®"""
        try:
            # ç§»é™¤ç©ºå€¼
            cleaned_df = df.filter(col("name").isNotNull() & col("phone").isNotNull())
            
            # æ ‡å‡†åŒ–å¹´é¾„å­—æ®µ
            cleaned_df = cleaned_df.withColumn(
                "age_clean", 
                when(col("age").isNull(), 0).otherwise(col("age"))
            )
            
            # æ ‡å‡†åŒ–æ—¶è–ªå­—æ®µ
            cleaned_df = cleaned_df.withColumn(
                "hourly_rate_clean",
                when(col("hourly_rate").isNull(), 0.0).otherwise(col("hourly_rate"))
            )
            
            # æ ‡å‡†åŒ–è¯„åˆ†å­—æ®µ
            cleaned_df = cleaned_df.withColumn(
                "rating_clean",
                when(col("rating").isNull(), 0.0).otherwise(col("rating"))
            )
            
            # æ·»åŠ æ•°æ®è´¨é‡æ ‡è®°
            cleaned_df = cleaned_df.withColumn(
                "data_quality",
                when(col("name").isNotNull() & col("phone").isNotNull() & 
                     col("age_clean") > 0 & col("hourly_rate_clean") > 0, 1.0)
                .otherwise(0.5)
            )
            
            logger.info("âœ… æŠ¤å·¥æ•°æ®æ¸…æ´—å®Œæˆ")
            return cleaned_df
            
        except Exception as e:
            logger.error(f"âŒ æŠ¤å·¥æ•°æ®æ¸…æ´—å¤±è´¥: {str(e)}")
            return df
    
    def clean_crawled_data(self, df):
        """æ¸…æ´—çˆ¬è™«æ•°æ®"""
        try:
            # ç§»é™¤ç©ºå€¼
            cleaned_df = df.filter(col("title").isNotNull())
            
            # æ ‡å‡†åŒ–è–ªèµ„å­—æ®µ
            cleaned_df = cleaned_df.withColumn(
                "salary_avg_clean",
                when(col("salary_avg").isNull(), 0.0).otherwise(col("salary_avg"))
            )
            
            # æå–åœ°ç‚¹ä¿¡æ¯
            cleaned_df = cleaned_df.withColumn(
                "city",
                when(col("location").contains("åŒ—äº¬"), "åŒ—äº¬")
                .when(col("location").contains("ä¸Šæµ·"), "ä¸Šæµ·")
                .when(col("location").contains("å¹¿å·"), "å¹¿å·")
                .when(col("location").contains("æ·±åœ³"), "æ·±åœ³")
                .otherwise("å…¶ä»–")
            )
            
            # æå–æœåŠ¡ç±»å‹
            cleaned_df = cleaned_df.withColumn(
                "service_type",
                when(col("title").contains("å…»è€"), "å…»è€æŠ¤ç†")
                .when(col("title").contains("åº·å¤"), "åº·å¤æŠ¤ç†")
                .when(col("title").contains("åŒ»ç–—"), "åŒ»ç–—æŠ¤ç†")
                .otherwise("ç»¼åˆæŠ¤ç†")
            )
            
            logger.info("âœ… çˆ¬è™«æ•°æ®æ¸…æ´—å®Œæˆ")
            return cleaned_df
            
        except Exception as e:
            logger.error(f"âŒ çˆ¬è™«æ•°æ®æ¸…æ´—å¤±è´¥: {str(e)}")
            return df
    
    def create_features(self, df):
        """åˆ›å»ºç‰¹å¾å·¥ç¨‹"""
        try:
            # å­—ç¬¦ä¸²ç´¢å¼•åŒ–
            string_indexer = StringIndexer(
                inputCols=["gender", "service_type", "city"],
                outputCols=["gender_index", "service_type_index", "city_index"]
            )
            
            # ç‹¬çƒ­ç¼–ç 
            one_hot_encoder = OneHotEncoder(
                inputCols=["gender_index", "service_type_index", "city_index"],
                outputCols=["gender_vec", "service_type_vec", "city_vec"]
            )
            
            # ç‰¹å¾å‘é‡åŒ–
            feature_columns = ["age_clean", "hourly_rate_clean", "rating_clean", 
                             "gender_vec", "service_type_vec", "city_vec"]
            
            vector_assembler = VectorAssembler(
                inputCols=feature_columns,
                outputCol="features"
            )
            
            # åˆ›å»ºç®¡é“
            pipeline = Pipeline(stages=[string_indexer, one_hot_encoder, vector_assembler])
            
            # è®­ç»ƒç®¡é“
            model = pipeline.fit(df)
            features_df = model.transform(df)
            
            logger.info("âœ… ç‰¹å¾å·¥ç¨‹å®Œæˆ")
            return features_df
            
        except Exception as e:
            logger.error(f"âŒ ç‰¹å¾å·¥ç¨‹å¤±è´¥: {str(e)}")
            return df
    
    def calculate_statistics(self, df):
        """è®¡ç®—ç»Ÿè®¡ä¿¡æ¯"""
        try:
            stats = df.select(
                count("*").alias("total_count"),
                avg("age_clean").alias("avg_age"),
                avg("hourly_rate_clean").alias("avg_hourly_rate"),
                avg("rating_clean").alias("avg_rating"),
                min("hourly_rate_clean").alias("min_hourly_rate"),
                max("hourly_rate_clean").alias("max_hourly_rate")
            ).collect()[0]
            
            logger.info("âœ… ç»Ÿè®¡ä¿¡æ¯è®¡ç®—å®Œæˆ")
            return stats
            
        except Exception as e:
            logger.error(f"âŒ ç»Ÿè®¡ä¿¡æ¯è®¡ç®—å¤±è´¥: {str(e)}")
            return None
    
    def save_processed_data(self, df, table_name):
        """ä¿å­˜å¤„ç†åçš„æ•°æ®"""
        try:
            # ä¿å­˜åˆ°HDFS
            df.write \
                .mode("overwrite") \
                .parquet(f"{DATA_PATHS['PROCESSED_DATA']}/{table_name}")
            
            logger.info(f"âœ… æ•°æ®ä¿å­˜æˆåŠŸ: {table_name}")
            
        except Exception as e:
            logger.error(f"âŒ æ•°æ®ä¿å­˜å¤±è´¥: {str(e)}")
    
    def process_all_data(self):
        """å¤„ç†æ‰€æœ‰æ•°æ®"""
        try:
            logger.info("ğŸš€ å¼€å§‹æ•°æ®å¤„ç†æµç¨‹")
            
            # åŠ è½½æ•°æ®
            caregiver_df = self.load_caregiver_data()
            user_df = self.load_user_data()
            appointment_df = self.load_appointment_data()
            crawled_df = self.load_crawled_data()
            
            if caregiver_df:
                # æ¸…æ´—æŠ¤å·¥æ•°æ®
                cleaned_caregiver_df = self.clean_caregiver_data(caregiver_df)
                
                # åˆ›å»ºç‰¹å¾
                features_caregiver_df = self.create_features(cleaned_caregiver_df)
                
                # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
                stats = self.calculate_statistics(cleaned_caregiver_df)
                logger.info(f"æŠ¤å·¥æ•°æ®ç»Ÿè®¡: {stats}")
                
                # ä¿å­˜å¤„ç†åçš„æ•°æ®
                self.save_processed_data(features_caregiver_df, "caregiver_features")
            
            if crawled_df:
                # æ¸…æ´—çˆ¬è™«æ•°æ®
                cleaned_crawled_df = self.clean_crawled_data(crawled_df)
                
                # ä¿å­˜å¤„ç†åçš„æ•°æ®
                self.save_processed_data(cleaned_crawled_df, "crawled_data_clean")
            
            logger.info("âœ… æ•°æ®å¤„ç†æµç¨‹å®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ æ•°æ®å¤„ç†æµç¨‹å¤±è´¥: {str(e)}")
    
    def close(self):
        """å…³é—­Sparkä¼šè¯"""
        if self.spark:
            self.spark.stop()
            logger.info("âœ… Sparkä¼šè¯å·²å…³é—­")

def main():
    """ä¸»å‡½æ•°"""
    processor = SparkDataProcessor()
    try:
        processor.process_all_data()
    finally:
        processor.close()

if __name__ == "__main__":
    main()
